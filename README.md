# Info-Retrieval-System
Overview

This project is a Retrieval-Augmented Generation (RAG) system built with LangChain, FAISS, and Streamlit.
It allows users to upload PDF documents and ask questions. The system retrieves relevant text from the PDFs and generates answers using Google Gemini (Generative AI).

âœ… Inspired by PaLM-based tutorials, but updated to Gemini API (PaLM is deprecated).
âœ… Maintains conversation history using LangChainâ€™s ConversationBufferMemory.
âœ… Uses FAISS vector store for fast similarity search.

ğŸ› ï¸ Tech Stack

Python

Streamlit (UI)

LangChain (chains, memory, retrieval)

FAISS (vector database)

Google Generative AI (Gemini) â€“ for LLM and embeddings

Sentence-Transformers (optional, local embeddings alternative)

âš™ï¸ Features

ğŸ“‚ Upload multiple PDF files.

ğŸ” Extracts text and splits into chunks.

ğŸ§  Stores embeddings in FAISS for similarity search.

ğŸ’¬ Chatbot answers based on retrieved context.

ğŸ“ Keeps full chat history during a session.

ğŸ”‘ Setup & Installation
# Clone this repository
git clone https://github.com/your-username/info-retrieval-system.git
cd info-retrieval-system

# Create and activate virtual environment
python -m venv genai
source genai/Scripts/activate   # (Windows Git Bash)

# Install dependencies
pip install -r requirements.txt


Create a .env file in the project root:

GOOGLE_API_KEY=your_api_key_here


Run the app:

streamlit run app.py

ğŸ“Œ Notes

This project originally used PaLM, but since PaLM is deprecated, it now uses Gemini (ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings).

Free Gemini API keys may have 0 quota for embeddings/generation unless billing is enabled.

As an alternative, you can use Sentence-Transformers (local embeddings) to run fully free/offline.

ğŸ“š Project Flow (RAG)

PDF Upload â†’ Extract text with PyPDF2.

Text Chunking â†’ Split into smaller pieces.

Embeddings â†’ Convert chunks into vectors (Gemini or Sentence-Transformers).

FAISS Vector Store â†’ Store and retrieve similar chunks.

Conversational Chain â†’ LLM (Gemini) generates answers with context.

Memory â†’ Stores all previous Q&A for chat continuity.
