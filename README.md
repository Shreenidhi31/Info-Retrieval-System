# Info-Retrieval-System
Overview

This project is a Retrieval-Augmented Generation (RAG) system built with LangChain, FAISS, and Streamlit.
It allows users to upload PDF documents and ask questions. The system retrieves relevant text from the PDFs and generates answers using Google Gemini (Generative AI).

âœ… Inspired by PaLM-based tutorials, but updated to Gemini API (PaLM is deprecated).
âœ… Maintains conversation history using LangChainâ€™s ConversationBufferMemory.
âœ… Uses FAISS vector store for fast similarity search.

ğŸ› ï¸ Tech Stack

Python

Streamlit (UI)

LangChain (chains, memory, retrieval)

FAISS (vector database)

Google Generative AI (Gemini) â€“ for LLM and embeddings

Sentence-Transformers (optional, local embeddings alternative)

âš™ï¸ Features

ğŸ“‚ Upload multiple PDF files.

ğŸ” Extracts text and splits into chunks.

ğŸ§  Stores embeddings in FAISS for similarity search.

ğŸ’¬ Chatbot answers based on retrieved context.

ğŸ“ Keeps full chat history during a session.


Run the app:
streamlit run app.py

ğŸ“š Project Flow (RAG)

PDF Upload â†’ Extract text with PyPDF2.

Text Chunking â†’ Split into smaller pieces.

Embeddings â†’ Convert chunks into vectors (Gemini or Sentence-Transformers).

FAISS Vector Store â†’ Store and retrieve similar chunks.

Conversational Chain â†’ LLM (Gemini) generates answers with context.

Memory â†’ Stores all previous Q&A for chat continuity.
